{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this produces the datasets used by the NN models.  It has various hyperparamaters which affect the y labels.\n",
    "\n",
    "There are two separate x input sets  : token vectors  (\\_1) & a binary feature vector indicating the token to classify (\\_2)\n",
    "\n",
    "The hyper parameters for the label matrices are described in the filename for the y matrices\n",
    "\n",
    "The y labels are used by both the CNN and UTH-CCB, ie the labels are the same for both models, only the inputs are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noel/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from datetime import datetime, timedelta\n",
    "import gensim\n",
    "import negation_data\n",
    "\n",
    "data_root_folder = 'data'\n",
    "data_raw_folder = data_root_folder + '/raw'\n",
    "data_processed_folder = data_root_folder + '/processed'\n",
    "data_cached_folder= data_root_folder + '/cached'\n",
    "\n",
    "# 8=referral\n",
    "# 9=primary\n",
    "experiment=9\n",
    "\n",
    "# study clinical finding tokens\n",
    "study_phrases=negation_data.get_study_phrases()\n",
    "\n",
    "# these labels match https://hhsrvmlr1.rvc.ac.uk:8888/notebooks/fp/False%20positives%20-%20CNN%20-%20Dataset%20production.ipynb\n",
    "class_labels={\n",
    "    0: '1-never diagnosed',\n",
    "    1: '2-prior to diagnostic window',\n",
    "    2: '3-after diagnostic window',\n",
    "    3: '4-during diagnostic window'}\n",
    "\n",
    "\n",
    "# hyper params\n",
    "embedding_dimension=300 # size of token vectors\n",
    "max_document_length=300 # truncate sentences longer than this\n",
    "adjust_diagnosis_date_by_days = 0 # number of days backwards in time to adjust the diagnosis date by\n",
    "window_before_diagnosis = 8 # number of hours before diagnosis to start the diagnosis window label (exclusive)\n",
    "window_after_diagnosis = 0 # number of hours after diagnosis to end the diagnosis window label (inclusive)\n",
    "number_of_classes=4 # controls the number of classes of y labels, choose amongst 2,3,4\n",
    "\n",
    "def get_tokeniser(file_name): #eg filename='negation_detection_sentences_experiment_8_train.txt'\n",
    "    def get_sentences_generator():\n",
    "        df=pd.read_csv(data_processed_folder + '/' + file_name, sep=',',header=0)\n",
    "    \n",
    "        for index, row in df.iterrows():\n",
    "            StudyId,PatientID,NoteID,CaseLabel,Sublabel,SentenceLabel,DiagnosisDate,SourceNoteRecordedDate,Sentence = row\n",
    "            yield Sentence\n",
    "    tokeniser=Tokenizer()\n",
    "    tokeniser.fit_on_texts(get_sentences_generator())\n",
    "    return tokeniser\n",
    "\n",
    "def make_study_token_vectors(study_phrases,tokens):\n",
    "    import re\n",
    "    \n",
    "    # implementation is not particularly efficient..\n",
    "    \n",
    "    # need to know when token spans start and end so we can compare without put of regex\n",
    "    token_spans=list() # (token,token span start, token span end)\n",
    "    token_start_index=0\n",
    "    for token in tokens:\n",
    "        token_start=token_start_index\n",
    "        token_length=len(token)\n",
    "        token_end=token_length + token_start # regex spans are inclusive of first position and exclusive of last position, ie 2 characters at start is span(0,2)\n",
    "        token_spans.append((token,token_start,token_end))\n",
    "        token_start_index=token_start_index + token_length + 1# one is whitespace between tokens\n",
    "    \n",
    "    # now match all the regex to the sentence\n",
    "    sentence = ' '.join(tokens)\n",
    "    \n",
    "    study_token_vector=np.zeros((len(tokens),1),dtype='float32')\n",
    "    \n",
    "    for study_phrase in study_phrases:\n",
    "        match=re.search(study_phrase, sentence)\n",
    "        if not match:\n",
    "            continue\n",
    "        span_start,span_end=match.span()\n",
    "        \n",
    "        for i,(token,token_start,token_end) in enumerate(token_spans):\n",
    "            if token_start >= span_start and token_end <= span_end:\n",
    "                study_token_vector[i]=1.0\n",
    "        break # only indicate the first study phrase (as there maybe more than on)\n",
    "            \n",
    "    return study_token_vector\n",
    "        \n",
    "def sentences_to_dataset(tokeniser,file_name):#eg filename='negation_detection_sentences_experiment_8_train.txt'\n",
    "    df=pd.read_csv(data_processed_folder + '/' + file_name, sep=',',header=0)\n",
    "    x=list()\n",
    "    x_study_token_vectors=list()\n",
    "    y=list()\n",
    "    for index, row in df.iterrows():\n",
    "        StudyId,PatientID,NoteID,CaseLabel,Sublabel,SentenceLabel,DiagnosisDate,SourceNoteRecordedDate,Sentence = row\n",
    "        \n",
    "        # create input 1. Tokens as integer (for embedding layer)\n",
    "        sequences=tokeniser.texts_to_sequences([Sentence])\n",
    "        x_i=np.concatenate(sequences) # tokeniser outputs sentences, jsut concat them back together\n",
    "        x.append(x_i)\n",
    "        \n",
    "        # create input 2. binary feature vector 1 = token in study phrase, 0 = token not in study phrase\n",
    "        phrases=study_phrases[str(StudyId).lower()]\n",
    "        tokens=list([embedding_to_token_map[embedding_id] for embedding_id in x_i])\n",
    "        binary_feature_vector=make_study_token_vectors(phrases,tokens)\n",
    "        x_study_token_vectors.append(binary_feature_vector)\n",
    "        \n",
    "        # now create label vector\n",
    "        \n",
    "        # 3 alternate labeling methids, 2 commented out at any 1 time\n",
    "        \n",
    "        if number_of_classes == 2:\n",
    "            # binary labels\n",
    "            # 0/1 FP or not\n",
    "            y_i = np.full((1),int(SentenceLabel))\n",
    "            y.append(y_i)\n",
    "        elif number_of_classes == 3:\n",
    "            # 3 classes\n",
    "            # 0 = never diagnosed\n",
    "            # 1 = sentence written before dx\n",
    "            # 2 = sentence written after dx\n",
    "            if pd.isnull(DiagnosisDate):\n",
    "                y.append(0)\n",
    "            else:\n",
    "                # pd currently has the dates as string.\n",
    "                # see this for conversion https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior \n",
    "                date_format='%d/%m/%Y %X %p'\n",
    "                dx_date=datetime.strptime(DiagnosisDate,date_format)\n",
    "                note_date=datetime.strptime(SourceNoteRecordedDate,date_format)\n",
    "\n",
    "                # adjust diagnosis date \n",
    "                dx_date=dx_date-timedelta(days=adjust_diagnosis_date_by_days)\n",
    "\n",
    "                if note_date < dx_date:\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(2)\n",
    "        elif number_of_classes == 4:\n",
    "             # 4 classes\n",
    "        \n",
    "            # 0 = never diagnosed\n",
    "            # 1 = sentence written before diagnosis window\n",
    "            # 2 = sentence written after diagnosis window\n",
    "            # 3 = sentence written during diagnosis window\n",
    "            if pd.isnull(DiagnosisDate):\n",
    "                y.append(0)\n",
    "            else:\n",
    "                # pd currently has the dates as string.\n",
    "                # see this for conversion https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior \n",
    "                date_format='%d/%m/%Y %X %p'\n",
    "                dx_date=datetime.strptime(DiagnosisDate,date_format)\n",
    "                note_date=datetime.strptime(SourceNoteRecordedDate,date_format)\n",
    "\n",
    "                # adjust diagnosis date \n",
    "                dx_date_window_start=dx_date-timedelta(hours=window_before_diagnosis)\n",
    "                dx_date_window_end=dx_date+timedelta(hours=window_after_diagnosis)\n",
    "\n",
    "                #written before diagnosis window starts\n",
    "                if note_date < dx_date_window_start:\n",
    "                    y.append(1)\n",
    "                #written after diagnosis window ends\n",
    "                elif note_date >= dx_date_window_end:\n",
    "                    y.append(2)\n",
    "                #written during diagnosis window\n",
    "                else:\n",
    "                    y.append(3)\n",
    "\n",
    "       \n",
    "    # pre process\n",
    "    x = pad_sequences(x,maxlen=max_document_length)\n",
    "    x_study_token_vectors = pad_sequences(x_study_token_vectors,maxlen=max_document_length)\n",
    "    lb=LabelBinarizer()\n",
    "    y=lb.fit_transform(y)\n",
    "    \n",
    "    x = np.stack(x)\n",
    "    x_study_token_vectors = np.stack(x_study_token_vectors)\n",
    "    y = np.stack(y)\n",
    "    return x,x_study_token_vectors,y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec_new():\n",
    "    return gensim.models.KeyedVectors.load_word2vec_format('data/processed/vec_sg_20180321.txt', binary=False, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noel/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:19: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "#load tokeniser & embeddings\n",
    "def get_tokeniser():\n",
    "    def get_sentences_generator():\n",
    "        df=pd.read_csv(data_processed_folder + '/' + 'negation_detection_sentences_experiment_8_train.txt', sep=',',header=0)\n",
    "    \n",
    "        for index, row in df.iterrows():\n",
    "            StudyId,PatientID,NoteID,CaseLabel,Sublabel,SentenceLabel,DiagnosisDate,SourceNoteRecordedDate,Sentence = row\n",
    "            yield Sentence\n",
    "        df=pd.read_csv(data_processed_folder + '/' + 'negation_detection_sentences_experiment_9_train.txt', sep=',',header=0)\n",
    "    \n",
    "        for index, row in df.iterrows():\n",
    "            StudyId,PatientID,NoteID,CaseLabel,Sublabel,SentenceLabel,DiagnosisDate,SourceNoteRecordedDate,Sentence = row\n",
    "            yield Sentence\n",
    "    tokeniser=Tokenizer()\n",
    "    tokeniser.fit_on_texts(get_sentences_generator())\n",
    "    \n",
    "    return tokeniser\n",
    "\n",
    "tokeniser=get_tokeniser()\n",
    "\n",
    "# inverse lookup embedding integer -> token\n",
    "embedding_to_token_map = {v: k for k, v in tokeniser.word_index.items()}\n",
    "embedding_to_token_map[0]='NULL'\n",
    "\n",
    "num_token_types=len(tokeniser.word_index)\n",
    "sg=load_word2vec_new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentence data and vectorise it\n",
    "x_train,x_train_2,y_train  = sentences_to_dataset(tokeniser,file_name='negation_detection_sentences_experiment_%s_train.txt' % experiment)\n",
    "x_dev,x_dev_2,y_dev        = sentences_to_dataset(tokeniser,file_name='negation_detection_sentences_experiment_%s_dev.txt' % experiment)\n",
    "x_test,x_test_2,y_test     = sentences_to_dataset(tokeniser,file_name='negation_detection_sentences_experiment_%s_test.txt' % experiment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def persist(x_1,x_2,y,dataset='train'):\n",
    "    np.save('data/datasets/x_%s_1_exp_%s.npy' % (dataset,experiment), x_1)\n",
    "    np.save('data/datasets/x_%s_2_exp_%s.npy' % (dataset,experiment), x_2)\n",
    "    np.save('data/datasets/y_%s_exp_%s_classes_%s_window_before_diagnosis_%s_window_after_diagnosis_%s.npy' % \n",
    "            (dataset,experiment,number_of_classes,window_before_diagnosis,window_after_diagnosis),y)\n",
    "\n",
    "persist(x_train, x_train_2, y_train,'train')\n",
    "persist(x_dev  , x_dev_2  , y_dev,'dev')\n",
    "persist(x_test , x_test_2 , y_test,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i used this to check that the sentence labels were being calculated accroding to the method in the paper.\n",
    "def get_class(y):\n",
    "    for i in range(0,number_of_classes):\n",
    "        if y[i]==1:\n",
    "            return i\n",
    "      \n",
    "def smoke_test_classifications():\n",
    "    # these are the datasets produced by the method\n",
    "    x_dev,x_dev_2,y_dev        = sentences_to_dataset(tokeniser,file_name='negation_detection_sentences_experiment_%s_dev.txt' % experiment)\n",
    "    \n",
    "    # lets compare them back to the source file\n",
    "    file_name='negation_detection_sentences_experiment_%s_dev.txt' % experiment\n",
    "    df=pd.read_csv(data_processed_folder + '/' + file_name, sep=',',header=0)\n",
    "    for index, row in df.iterrows():\n",
    "        StudyId,PatientID,NoteID,CaseLabel,Sublabel,SentenceLabel,DiagnosisDate,SourceNoteRecordedDate,Sentence = row\n",
    "        sentence_class = get_class(y_dev[index])\n",
    "        print('*** %s ***' %class_labels[sentence_class])\n",
    "        print('Dx=%s DocumentDate=%s'%(DiagnosisDate,SourceNoteRecordedDate))\n",
    "        print(Sentence)\n",
    "        zz=four_class_label(DiagnosisDate,SourceNoteRecordedDate)\n",
    "        if index == 20:\n",
    "            return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
